{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "So far you've predicted numeric targets. This type of modeling is called **regression**, hence the \"Regressor\" part of `RandomForestRegressor`.\n",
    "\n",
    "Another common problem you'll see is making a choice between mutually exclusive outcomes. For example, spam detection is predicting whether an email is \"spam\" or \"not spam\" based on the email's content. This type of modeling is called **classification**.\n",
    "\n",
    "There are two types of classification: binary (choosing between two classes) and multiclass (choosing between more than two classes). In general there are different approaches to the two types of classification, but most multiclass models will also work for binary problems.\n",
    "\n",
    "It's straightforward to build classification models using what you already know about scikit-learn. Instead of `RandomForestRegressor`, you will use `RandomForestClassifier`. \n",
    "\n",
    "As an example of classification with `RandomForestClassifier`, I'll use a dataset of phone features to predict a phone's price range. The targets in the data have values:\n",
    "\n",
    " * 0 (low cost)\n",
    " * 1 (medium cost)\n",
    " * 2 (high cost)\n",
    " * 3 (very high cost)\n",
    " \n",
    "The features are things like\n",
    "\n",
    "* battery_power: Total energy a battery can store in one time measured in mAh\n",
    "* blue: Has bluetooth or not\n",
    "* clock_speed: speed at which microprocessor executes instructions\n",
    "* dual_sim: Has dual sim support or not\n",
    "* fc: Front Camera mega pixels\n",
    "* four_g: Has 4G or not\n",
    "* ....\n",
    "\n",
    "Here is a quick overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "data = pd.read_csv('../input/mobile-price-classification/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our feature and targets the same as before using `train_test_split`. This part looks like what you've already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for the targets and features\n",
    "y = data['price_range']\n",
    "X = data.drop('price_range', axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting the model is similar to what you've done before, except you'll use `RandomForestClassifier` instead of `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier and fit it to our training data\n",
    "model = RandomForestClassifier(random_state=7, n_estimators=100)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest metric for classification models is the **accuracy**, the fraction predictions that are correct. Scikit-learn provides `metrics.accuracy_score` to calculate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes given the validation features\n",
    "pred_y = model.predict(val_X)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(val_y, pred_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Our model did pretty well, correctly predicting around 86% of the price ranges in the validation data. It's often useful to look at where the model is failing with a **confusion matrix** which shows us how our model classified the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix itself\n",
    "confusion = metrics.confusion_matrix(val_y, pred_y)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "\n",
    "# Normalizing by the true label counts to get rates\n",
    "print(f\"\\nNormalized confusion matrix:\")\n",
    "for row in confusion:\n",
    "    print(row / row.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little easier to understand as a nice little figure like so:\n",
    "\n",
    "<img src=\"https://i.imgur.com/idD0k8y.png\" alt=\"example confusion matrix\" width=400px>\n",
    "\n",
    "The rows of the confusion matrix are the true class and the columns are the predicted class. The diagonal tells us how many of each class the model predicted correctly. The off-diagonals show where the model is making wrong predictions, where it is \"confused.\" For example, looking at the first column and second row, we classified four phones that were actually low cost as medium cost. We see for classes 0 and 3, the low cost and highest cost phones, our model works really well, above 90% accurate. However, our model is weaker for medium and high cost phones. Note that incorrect predictions are only between adjacent classes. The model doesn't confuse low cost and very high cost phones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class probabilities \n",
    "\n",
    "Classification models actually calculate a *probability distribution* over the classes. Using `model.predict` simply returns the class with the highest probability. This might not be ideal based on how the decision affects your metrics or downstream measures. To get the probabilities themselves, use the `.predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(val_X)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the probability the model assigns to each class. Often in business problems, decisions you make lead to different monetary returns. The expected return for a decision based on your classifier is the probability times the monetary return of that decision.\n",
    "\n",
    "Consider probabilities `[0.05 0.17 0.42 0.36]`. Assume the third option would result in \\\\$100 of profit while the fourth option would return \\\\$150 in profit. Then the expected monetary values are $0.42* \\$100 = \\$42$ and $0.36*\\$150 = \\$54$. Even though the third option has the highest probability, on average it would be better from a business perspective to choose the fourth option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn \n",
    "Try **[some classification](#$NEXT_NOTEBOOK_URL$)** yourself. It's not complicated given what you already know, and it will dramatically expand what types of use cases you can tackle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
